{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TF_CPP_MIN_LOG_LEVEL=2 TF_CPP_MIN_LOG_LEVEL=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 19:42:11.912596: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2025-05-05 19:42:13.597199: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-05-05 19:42:13.597971: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2025-05-05 19:42:13.710066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-05 19:42:13.712609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA A30 computeCapability: 8.0\n",
      "coreClock: 1.44GHz coreCount: 56 deviceMemorySize: 23.49GiB deviceMemoryBandwidth: 869.04GiB/s\n",
      "2025-05-05 19:42:13.712625: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2025-05-05 19:42:13.714460: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2025-05-05 19:42:13.714492: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2025-05-05 19:42:13.715237: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2025-05-05 19:42:13.715414: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2025-05-05 19:42:13.717263: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2025-05-05 19:42:13.717645: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2025-05-05 19:42:13.717744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2025-05-05 19:42:13.717826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-05 19:42:13.720198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-05 19:42:13.722518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2025-05-05 19:42:13.722868: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-05 19:42:13.723825: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2025-05-05 19:42:13.723915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-05 19:42:13.726242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA A30 computeCapability: 8.0\n",
      "coreClock: 1.44GHz coreCount: 56 deviceMemorySize: 23.49GiB deviceMemoryBandwidth: 869.04GiB/s\n",
      "2025-05-05 19:42:13.726256: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2025-05-05 19:42:13.726268: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2025-05-05 19:42:13.726277: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2025-05-05 19:42:13.726286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2025-05-05 19:42:13.726294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2025-05-05 19:42:13.726303: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2025-05-05 19:42:13.726313: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2025-05-05 19:42:13.726321: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2025-05-05 19:42:13.726374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-05 19:42:13.728735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-05 19:42:13.731036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2025-05-05 19:42:13.731060: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2025-05-05 19:42:14.210262: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2025-05-05 19:42:14.210297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2025-05-05 19:42:14.210304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2025-05-05 19:42:14.210522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-05 19:42:14.212297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-05 19:42:14.213855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-05 19:42:14.215374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22251 MB memory) -> physical GPU (device: 0, name: NVIDIA A30, pci bus id: 0000:01:00.0, compute capability: 8.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 144, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "rescaling (Rescaling)        (None, 144, 256, 3)       0         \n",
      "_________________________________________________________________\n",
      "normalization (Normalization (None, 144, 256, 3)       7         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 70, 126, 24)       1824      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 33, 61, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 29, 48)        43248     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 27, 64)        27712     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 13, 16)         9232      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1248)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               159872    \n",
      "=================================================================\n",
      "Total params: 263,531\n",
      "Trainable params: 263,524\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense\n",
    "import argparse\n",
    "import copy\n",
    "import json\n",
    "import os.path\n",
    "from enum import Enum\n",
    "from typing import Dict, Tuple, Union, Optional, Any\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from PIL import Image\n",
    "from typing import List, Iterable, Optional, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from numpy import ndarray\n",
    "from tensorflow import keras, Tensor\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.models import Functional\n",
    "from keras_models import generate_ncp_model\n",
    "\n",
    "IMAGE_SHAPE = (144, 256, 3)\n",
    "IMAGE_SHAPE_CV = (IMAGE_SHAPE[1], IMAGE_SHAPE[0])\n",
    "\n",
    "DEFAULT_NCP_SEED = 22222\n",
    "\n",
    "batch_size = None\n",
    "seq_len = 64\n",
    "augmentation_params = None\n",
    "single_step = True\n",
    "no_norm_layer = False\n",
    "mymodel = generate_ncp_model(seq_len, IMAGE_SHAPE, augmentation_params, batch_size, DEFAULT_NCP_SEED, single_step, no_norm_layer)\n",
    "\n",
    "# pretrained model weights\n",
    "# mymodel.load_weights('model-ncp-val.hdf5')\n",
    "\n",
    "# custom model weights\n",
    "root = \"./retrain_150traj_wscheduler0.85_seed22222_lr0.001_trainloss0.00035_valloss0.00019_coreset900.h5\"\n",
    "mymodel.load_weights(root)\n",
    "\n",
    "conv_layers = [layer for layer in mymodel.layers if isinstance(layer, Conv2D)]\n",
    "\n",
    "# Get the last Conv2D layer\n",
    "last_conv = conv_layers[-1]\n",
    "\n",
    "# Get the next two layers after the last Conv2D\n",
    "layer_dict = {layer.name: layer for layer in mymodel.layers}\n",
    "next_layer_1 = None\n",
    "next_layer_2 = None\n",
    "\n",
    "found = False\n",
    "for i, layer in enumerate(mymodel.layers):\n",
    "    if layer.name == last_conv.name:\n",
    "        next_layer_1 = mymodel.layers[i + 1]\n",
    "        next_layer_2 = mymodel.layers[i + 2]\n",
    "        break\n",
    "\n",
    "# Sanity check\n",
    "assert isinstance(next_layer_1, Flatten), f\"Expected Flatten, got {type(next_layer_1)}\"\n",
    "assert isinstance(next_layer_2, Dense), f\"Expected Dense, got {type(next_layer_2)}\"\n",
    "\n",
    "# Create new visualization model\n",
    "vis_model = Model(\n",
    "    inputs=mymodel.inputs[0],\n",
    "    outputs=[last_conv.output, next_layer_1.output, next_layer_2.output]\n",
    ")\n",
    "\n",
    "print(vis_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hidden_list(model: Functional, return_numpy: bool = True):\n",
    "    \"\"\"\n",
    "    Generates a list of tensors that are used as the hidden state for the argument model when it is used in single-step\n",
    "    mode. The batch dimension (0th dimension) is assumed to be 1 and any other dimensions (seq len dimensions) are\n",
    "    assumed to be 0\n",
    "\n",
    "    :param return_numpy: Whether to return output as numpy array. If false, returns as keras tensor\n",
    "    :param model: Single step functional model to infer hidden states for\n",
    "    :return: list of hidden states with 0 as value\n",
    "    \"\"\"\n",
    "    constructor = np.zeros if return_numpy else tf.zeros\n",
    "    hiddens = []\n",
    "    print(\"Length of model input shape: \", len(model.input_shape))\n",
    "    if len(model.input_shape)==1:\n",
    "        lool = model.input_shape[0][1:]\n",
    "    else:\n",
    "        print(\"model input shape: \", model.input_shape)\n",
    "        lool = model.input_shape[1:]\n",
    "    print(\"lool: \", lool)\n",
    "    for input_shape in lool:  # ignore 1st output, as is this control output\n",
    "        hidden = []\n",
    "        for i, shape in enumerate(input_shape):\n",
    "            if shape is None:\n",
    "                if i == 0:  # batch dim\n",
    "                    hidden.append(1)\n",
    "                    continue\n",
    "                elif i == 1:  # seq len dim\n",
    "                    hidden.append(0)\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Unable to infer hidden state shape. Leaving as none\")\n",
    "            hidden.append(shape)\n",
    "        hiddens.append(constructor(hidden))\n",
    "    return hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'PIL.PngImagePlugin.PngImageFile'>\"}), <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m pth \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset/1/Image1.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m load_img(pth)\n\u001b[0;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mvis_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m latent_vector \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(latent_vector)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1598\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1592\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1593\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing Model.predict with \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1594\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1595\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1596\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1598\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py:1099\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1096\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1097\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m-> 1099\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m \u001b[43mselect_data_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m adapter_cls(\n\u001b[1;32m   1101\u001b[0m     x,\n\u001b[1;32m   1102\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     distribution_strategy\u001b[38;5;241m=\u001b[39mds_context\u001b[38;5;241m.\u001b[39mget_strategy(),\n\u001b[1;32m   1112\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   1114\u001b[0m strategy \u001b[38;5;241m=\u001b[39m ds_context\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py:961\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    958\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m    960\u001b[0m   \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m--> 961\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    962\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    963\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    964\u001b[0m           _type_name(x), _type_name(y)))\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    966\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    967\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    968\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    969\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    970\u001b[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'PIL.PngImagePlugin.PngImageFile'>\"}), <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "def load_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize(IMAGE_SHAPE_CV)\n",
    "    img_array = np.array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = tf.convert_to_tensor(img_array)\n",
    "    return img_array\n",
    "\n",
    "pth = \"dataset/1/Image1.png\"\n",
    "img = load_img(pth)\n",
    "output = vis_model.predict([img])\n",
    "latent_vector = output[2]\n",
    "print(latent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: 1\n",
      "dataset/1/Image1.png\n",
      "(1, 6, 13, 16)\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"dataset\"\n",
    "IMAGE_SHAPE = (144, 256, 3)\n",
    "IMAGE_SHAPE_CV = (IMAGE_SHAPE[1], IMAGE_SHAPE[0])\n",
    "\n",
    "def load_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize(IMAGE_SHAPE_CV)\n",
    "    img_array = np.array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = tf.convert_to_tensor(img_array)\n",
    "    return img_array\n",
    "\n",
    "\n",
    "latent_space_data = []\n",
    "\n",
    "for dir_index in range(len(os.listdir(base_dir))):\n",
    "    print(\"Processing directory:\", dir_index + 1)\n",
    "    folder_path = os.path.join(base_dir, str(dir_index + 1))\n",
    "\n",
    "    # hiddens = generate_hidden_list(model=model, return_numpy=True)\n",
    "    files_length = len([file for file in os.listdir(folder_path) if '.png' in file])\n",
    "\n",
    "    for frame_index in range(files_length - 1):\n",
    "        image_path = f\"{folder_path}/Image{frame_index + 1}.png\"\n",
    "        img = load_image(image_path)\n",
    "        print(image_path)\n",
    "\n",
    "        output = vis_model.predict([img])\n",
    "        latent_vector = output[2]\n",
    "        # hiddens = output[1:]\n",
    "        # preds = output[0][0]\n",
    "        # vx, vy, vz, omega_z = preds[0], preds[1], preds[2], preds[3]\n",
    "\n",
    "        # prediction_data.append([vx, vy, vz, omega_z])\n",
    "        break\n",
    "    break\n",
    "\n",
    "df = pd.DataFrame(prediction_data, columns=['vx', 'vy', 'vz', 'omega_z'])\n",
    "\n",
    "# Save to CSV\n",
    "# df.to_csv(\"prediction_output.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
